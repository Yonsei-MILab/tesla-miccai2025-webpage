<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TESLA: Test-time Reference-free Through-plane Super-resolution for Multi-contrast Brain MRI.">
  <meta name="keywords" content="Super-resolution, Contrastive learning, Multi-contrastMRI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TESLA: Test-time Reference-free Through-plane Super-resolution for Multi-contrast Brain MRI</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="css/bulma.min.css">
  <link rel="stylesheet" href="css/bulma-carousel.min.css">
  <link rel="stylesheet" href="css/bulma-slider.min.css">
  <link rel="stylesheet" href="css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="css/index.css">
  <link rel="icon" href="https://upload.wikimedia.org/wikipedia/commons/0/09/Flag_of_South_Korea.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="js/fontawesome.all.min.js"></script>
  <script src="js/bulma-carousel.min.js"></script>
  <script src="js/bulma-slider.min.js"></script>
  <script src="js/index.js"></script>
</head>

<body>
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sites.google.com/view/yoonseok-choi/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">TESLA: Test-time Reference-free Through-plane Super-resolution for Multi-contrast Brain MRI</h1>
          <h4 class="title is-4 publication-title">MICCAI 2025</h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/yoonseok-choi/">Yoonseok Choi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/sunyoung-jung/">Sunyoung Jung</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/miti-lab-almasni/">Mohammed A. Al-masni</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?user=zE4cGg0AAAAJ&hl=ko&oi=ao">Dong-Hyun Kim</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Electrical and Electronic Engineering, Yonsei University, Seoul, Republic of Korea,</span>
            <span class="author-block"><sup>2</sup>Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea</span>
            <span class="author-block"><sup>3</sup>Department of Artificial Intelligence and Data Science, Sejong University, Seoul, Republic of Korea</span>
            <span class="author-block"><sup>4</sup>Electrical Engineering and Computer Science, University of California at Merced, Merced, USA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
                <!-- PDF Link. -->
              <!--
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              -->
              
              <!-- Video Link. -->
              <!--
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Yonsei-MILab/TESLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- TESLA Framework. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

        <!-- TESLA Framework. -->
        <div class="columns is-vcentered TESLA Framework">
          <div class="column is-12 has-text-centered">
            <img src="images\Fig.2_Framework_600dpi_100_light_blue_bg.jpg"
                 class="TESLA Framework"
                 alt="TESLA Framework."/>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Fig. 1.</b> Overview of the proposed network TESLA.
            In the first stage, we progressively reconstruct LR Tar.
            In the second stage, we leverage the high-quality content information
            disentangled from HR Ref with the pre-trained ContentNet to enrich the structural
            fine detail of Soft SR Tar with the patch-wise contrastive learning. HR Ref and ContentNet
            are used only during training; inference requires only PR, the encoders (\(E_c\)
            and \(E_s\)), and decoder (\(G\)) in SE.
          </p>
        </div>
        <br/>
        <!--/ TESLA Framework. -->
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Through-plane super-resolution (SR) in brain magnetic resonance
            imaging (MRI) is clinically important during clinical assessments.
            Most existing multi-contrast SR models mainly focus on enhancing inplane
            image resolution, relying on functions already integrated into MRI
            scanners. These methods usually leverage proprietary fusion techniques
            to integrate multi-contrast images, resulting in diminished interpretability.
            Furthermore, the requirement for reference images during testing
            limits their applicability in clinical settings. We propose a TEst time
            reference-free through-plane Super-resoLution network using disentAngled
            representation learning in multi-contrast MRI (TESLA) to address
            these challenges. Our method is developed on the premise that multicontrast
            images consist of shared content (structure) and independent
            stylistic (contrast) features. Thus, after progressively reconstructing the
            target image in the first stage, we divide it into shared and independent
            elements during the structure enhancement phase. In this stage, we
            employ a pre-trained ContentNet to effectively disentangle high-quality
            structural information from the reference image, enabling the shared
            components of the target image to learn directly from those of the reference
            image through patch-wise contrastive learning during training.
            Consequently, the proposed model enhances clinical applicability while
            ensuring model interpretability. Extensive experimental results demonstrate
            that the proposed model performs favorably against other state-ofthe-
            art multi-contrast SR models, especially in restoring structural fine
            details in the through-plane direction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
     <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Qualitative Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results</h2>

        <!-- Through-plane SR results. -->
        <div class="columns is-vcentered through-plane SR results">
          <div class="column is-12 has-text-centered">
            <img src="images\Fig.3_Comparison_TPSR_ixi_hcp_600dpi_100.jpg"
                 class="through-plane SR results"
                 alt="through-plane SR results on IXI and HCP dataset."/>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Fig. 2.</b> Qualitative comparison results of through-plane SR on IXI and HCP dataset.
          </p>
        </div>
        <br/>
        <!--/ Through-plane SR results. -->

        <!-- Pseudo-vessel recon results. -->
        <div class="columns is-vcentered pseudo-vessel recon results">
          <div class="column is-12 has-text-centered">
            <img src="images\Fig.4(a)_Comparison_pseudo_vessel_inhouse_COR_600dpi_100.jpg"
                 class="pseudo-vessel recon results"
                 alt="pseudo-vessel recon results on in-house dataset."/>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Fig. 3.</b> Qualitative comparison results of pseudo-vessel reconstruction on in-house 
            dataset when the scaling factor is ×4.
          </p>
        </div>
        <br/>
        <!--/ Pseudo-vessel recon results. -->

      </div>
    </div>
    <!--/ Qualitative Results. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Ablation study. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Ablation study</h2>

        <!-- loss combinations in ContentNet. -->
        <div class="columns is-vcentered the loss combinations in ContentNet">
          <div class="column is-12 has-text-centered">
            <img src="images\Fig.8_Ablation_contentNet_600dpi_100.jpg"
                 class="loss combinations in ContentNet"
                 alt="the loss combinations in ContentNet on the IXI dataset."/>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Fig. 4.</b> Qualitative results of the ablation study on the loss combinations in ContentNet
            on the IXI dataset, which effectively decomposes high-quality structural information
            from HR Ref. First row: MUNIT (L1 + Perceptual + Adversarial) + LSGAN, Second
            row: MUNIT (L1 + Perceptual + Adversarial) + PatchGAN, and Third row: MUNIT 
            (L1 + SSIM + Adversarial) + PatchGAN. \(c^{i}_{HR T1} (i = 1, 2, 3)\) indicates randomly
            selected content information decomposed from HR T1 on each condition.
          </p>
        </div>
        <br/>
        <!--/ loss combinations in ContentNet. -->

        <!-- Optimal reference modality. -->
        <div class="columns is-vcentered Optimal reference modality">
          <div class="column is-12 has-text-centered">
            <img src="images\Fig.9_Ablation_ixi_optimal_ref_SAG_600dpi_100.jpg"
                 class="Optimal reference modality"
                 alt="Optimal reference modality on the IXI dataset."/>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            <b>Fig. 5.</b> Qualitative results of the ablation study analyzing the optimal HR Ref on
            the IXI dataset when the scaling factor is ×4. \(x^{enhan}_{SRT2}\) means the final output of the
            proposed network on each condition. \(c^{i}_{SRT2} (i = 1, 2, 3)\) indicates randomly selected
            content information decomposed from SR T2 on each condition. \(x_{ref}\) denotes HD Ref.
          </p>
        </div>
        <br/>
        <!--/ Optimal reference modality. -->

      </div>
    </div>
    <!--/ Ablation study. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content has-text-centered">
    <h2 class="title">BibTeX</h2>
    <div class="content has-text-justified">
    <p>
    <pre><code>@article{choi2025tesla,
    author    = {Yoonseok Choi and Sunyoung Jung and Mohammed A. Al-masni and Ming-Hsuan Yang and Dong-Hyun Kim},
    title     = {TESLA: Test-time Reference-free Through-plane Super-resolution for Multi-contrast Brain MRI},
    booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
    year      = {2025},
    }</code></pre>
    </p>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      
      <a class="icon-link" href="https://github.com/yoonseokchoi-ai" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>

    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>